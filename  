[INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:57:18 >> âœ… VeOmni ops patch applied.
[INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:57:18 >> âŒ veomni_patch is not available
[INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:57:18 >> 
========== Environment Variables ==========
MODELING_BACKEND=veomni (source=default)
USE_GROUP_GEMM=1 (source=default)
USE_LIGER_KERNEL=1 (source=default)
===========================================
[INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:57:18 >> 
=========== OPS ============
_fused_moe_forward = npu_fused_moe_forward
_flash_attention_forward = transformers_flash_attention_forward
_cross_entropy = eager_cross_entropy
==============================
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Rank 0 | Local Rank 0] 2026-02-03 07:57:18,781 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-02-03 07:57:20,427][verl.utils.device][WARNING] - Detect setting config.trainer.device to cuda for Ascend NPU, maybefrom default value in config file, automatically set to `npu` instead.
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/ray/_private/node.py:1136: ResourceWarning: unclosed file <_io.TextIOWrapper name='/dev/null' mode='w' encoding='utf-8'>
  process_info = ray._private.services.start_gcs_server(
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/ray/_private/node.py:1097: ResourceWarning: unclosed file <_io.TextIOWrapper name='/dev/null' mode='w' encoding='utf-8'>
  self._webui_url, process_info = ray._private.services.start_api_server(
2026-02-03 07:57:26,260	INFO worker.py:1998 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
/root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(pid=2594856)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(pid=2594856)[0m     *************************************************************************************************************
[36m(pid=2594856)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(pid=2594856)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(pid=2594856)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(pid=2594856)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(pid=2594856)[0m     The device parameters have been replaced with npu in the function below:
[36m(pid=2594856)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(pid=2594856)[0m     *************************************************************************************************************
[36m(pid=2594856)[0m     
[36m(pid=2594856)[0m   warnings.warn(msg, ImportWarning)
[36m(pid=2594856)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(pid=2594856)[0m   warnings.warn(msg, RuntimeWarning)
[36m(pid=2594856)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(pid=2594856)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(pid=2594856)[0m   import pkg_resources
ray init kwargs: {'num_cpus': None, 'runtime_env': {'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN', 'VLLM_LOGGING_LEVEL': 'WARN', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'VLLM_ALLREDUCE_USE_SYMM_MEM': '0', 'CUDA_DEVICE_MAX_CONNECTIONS': '1', 'NCCL_CUMEM_ENABLE': '0'}, 'working_dir': None}}
[36m(pid=2594856)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:57:39 >> âœ… VeOmni ops patch applied.
[36m(pid=2594856)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:57:39 >> âŒ veomni_patch is not available
[36m(pid=2594856)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:57:39 >> 
[36m(pid=2594856)[0m ========== Environment Variables ==========
[36m(pid=2594856)[0m MODELING_BACKEND=veomni (source=default)
[36m(pid=2594856)[0m USE_GROUP_GEMM=1 (source=default)
[36m(pid=2594856)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(pid=2594856)[0m ===========================================
[36m(pid=2594856)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:57:39 >> 
[36m(pid=2594856)[0m =========== OPS ============
[36m(pid=2594856)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(pid=2594856)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(pid=2594856)[0m _cross_entropy = eager_cross_entropy
[36m(pid=2594856)[0m ==============================
[36m(pid=2594856)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:57:39,919 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(TaskRunner pid=2594856)[0m TaskRunner hostname: node-29-117, PID: 2594856
[36m(TaskRunner pid=2594856)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.VeOmniActorConfig',
[36m(TaskRunner pid=2594856)[0m                                  'calculate_entropy': False,
[36m(TaskRunner pid=2594856)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=2594856)[0m                                                 'async_save': False,
[36m(TaskRunner pid=2594856)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=2594856)[0m                                                                   'optimizer',
[36m(TaskRunner pid=2594856)[0m                                                                   'extra'],
[36m(TaskRunner pid=2594856)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=2594856)[0m                                                                   'optimizer',
[36m(TaskRunner pid=2594856)[0m                                                                   'extra']},
[36m(TaskRunner pid=2594856)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=2594856)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=2594856)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=2594856)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=2594856)[0m                                  'data_loader_seed': 42,
[36m(TaskRunner pid=2594856)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=2594856)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=2594856)[0m                                  'kl_loss_coef': 0.01,
[36m(TaskRunner pid=2594856)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=2594856)[0m                                  'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=2594856)[0m                                  'loss_scale_factor': None,
[36m(TaskRunner pid=2594856)[0m                                  'optim': {'_target_': 'verl.workers.config.VeOmniOptimizerConfig',
[36m(TaskRunner pid=2594856)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=2594856)[0m                                            'clip_grad': 1.0,
[36m(TaskRunner pid=2594856)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=2594856)[0m                                            'lr_decay_ratio': 1.0,
[36m(TaskRunner pid=2594856)[0m                                            'lr_min': 0.0,
[36m(TaskRunner pid=2594856)[0m                                            'lr_scheduler_type': 'cosine',
[36m(TaskRunner pid=2594856)[0m                                            'lr_start': 0.0,
[36m(TaskRunner pid=2594856)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=2594856)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2594856)[0m                                            'optimizer': 'adamw',
[36m(TaskRunner pid=2594856)[0m                                            'override_optimizer_config': {},
[36m(TaskRunner pid=2594856)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=2594856)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=2594856)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=2594856)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=2594856)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=2594856)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=2594856)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=2594856)[0m                                                  'loss_mode': 'vanilla',
[36m(TaskRunner pid=2594856)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=2594856)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=2594856)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2594856)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2594856)[0m                                  'ppo_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=2594856)[0m                                  'ppo_mini_batch_size': 4,
[36m(TaskRunner pid=2594856)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=2594856)[0m                                               'enable': False,
[36m(TaskRunner pid=2594856)[0m                                               'ranks': [],
[36m(TaskRunner pid=2594856)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                                               'tool': None,
[36m(TaskRunner pid=2594856)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=2594856)[0m                                                                       'contents': [],
[36m(TaskRunner pid=2594856)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                                       'level': 'level0'},
[36m(TaskRunner pid=2594856)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=2594856)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=2594856)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=2594856)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2594856)[0m                                  'rollout_n': 8,
[36m(TaskRunner pid=2594856)[0m                                  'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=2594856)[0m                                                    'mode': 'disabled',
[36m(TaskRunner pid=2594856)[0m                                                    'record_file': None,
[36m(TaskRunner pid=2594856)[0m                                                    'replay_file': None},
[36m(TaskRunner pid=2594856)[0m                                  'shuffle': False,
[36m(TaskRunner pid=2594856)[0m                                  'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                                  'tau_neg': 1.05,
[36m(TaskRunner pid=2594856)[0m                                  'tau_pos': 1.0,
[36m(TaskRunner pid=2594856)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=2594856)[0m                                  'use_prefix_grouper': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_torch_compile': False,
[36m(TaskRunner pid=2594856)[0m                                  'veomni': {'_target_': 'verl.workers.config.VeOmniEngineConfig',
[36m(TaskRunner pid=2594856)[0m                                             'activation_gpu_limit': 0.0,
[36m(TaskRunner pid=2594856)[0m                                             'attn_implementation': 'flash_attention_2',
[36m(TaskRunner pid=2594856)[0m                                             'ckpt_manager': 'dcp',
[36m(TaskRunner pid=2594856)[0m                                             'context_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'data_parallel_mode': 'fsdp2',
[36m(TaskRunner pid=2594856)[0m                                             'data_parallel_replicate_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'data_parallel_shard_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'data_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                             'enable_fsdp_offload': False,
[36m(TaskRunner pid=2594856)[0m                                             'enable_full_shard': True,
[36m(TaskRunner pid=2594856)[0m                                             'enable_reentrant': False,
[36m(TaskRunner pid=2594856)[0m                                             'expert_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                             'force_use_huggingface': False,
[36m(TaskRunner pid=2594856)[0m                                             'forward_only': False,
[36m(TaskRunner pid=2594856)[0m                                             'forward_prefetch': True,
[36m(TaskRunner pid=2594856)[0m                                             'full_determinism': False,
[36m(TaskRunner pid=2594856)[0m                                             'init_device': 'meta',
[36m(TaskRunner pid=2594856)[0m                                             'mixed_precision': True,
[36m(TaskRunner pid=2594856)[0m                                             'moe_implementation': 'fused',
[36m(TaskRunner pid=2594856)[0m                                             'optimizer_offload': True,
[36m(TaskRunner pid=2594856)[0m                                             'param_offload': True,
[36m(TaskRunner pid=2594856)[0m                                             'pipeline_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'seed': 42,
[36m(TaskRunner pid=2594856)[0m                                             'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                                             'tensor_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'ulysses_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                             'use_torch_compile': False}},
[36m(TaskRunner pid=2594856)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=2594856)[0m                        'model': {'_target_': 'verl.workers.config.HFModelConfig',
[36m(TaskRunner pid=2594856)[0m                                  'custom_chat_template': None,
[36m(TaskRunner pid=2594856)[0m                                  'enable_activation_offload': False,
[36m(TaskRunner pid=2594856)[0m                                  'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=2594856)[0m                                  'exclude_modules': None,
[36m(TaskRunner pid=2594856)[0m                                  'external_lib': None,
[36m(TaskRunner pid=2594856)[0m                                  'fused_kernel_options': {'impl_backend': 'torch'},
[36m(TaskRunner pid=2594856)[0m                                  'hf_config_path': None,
[36m(TaskRunner pid=2594856)[0m                                  'lora': {'a2a_experimental': False,
[36m(TaskRunner pid=2594856)[0m                                           'adapter_path': None,
[36m(TaskRunner pid=2594856)[0m                                           'alpha': 32,
[36m(TaskRunner pid=2594856)[0m                                           'dropout': 0.0,
[36m(TaskRunner pid=2594856)[0m                                           'dropout_position': 'pre',
[36m(TaskRunner pid=2594856)[0m                                           'dtype': None,
[36m(TaskRunner pid=2594856)[0m                                           'exclude_modules': [],
[36m(TaskRunner pid=2594856)[0m                                           'freeze_language_model': True,
[36m(TaskRunner pid=2594856)[0m                                           'freeze_vision_model': True,
[36m(TaskRunner pid=2594856)[0m                                           'freeze_vision_projection': True,
[36m(TaskRunner pid=2594856)[0m                                           'lora_A_init_method': 'xavier',
[36m(TaskRunner pid=2594856)[0m                                           'lora_B_init_method': 'zero',
[36m(TaskRunner pid=2594856)[0m                                           'rank': 0,
[36m(TaskRunner pid=2594856)[0m                                           'target_modules': ['linear_qkv',
[36m(TaskRunner pid=2594856)[0m                                                              'linear_proj',
[36m(TaskRunner pid=2594856)[0m                                                              'linear_fc1',
[36m(TaskRunner pid=2594856)[0m                                                              'linear_fc2'],
[36m(TaskRunner pid=2594856)[0m                                           'type': 'lora'},
[36m(TaskRunner pid=2594856)[0m                                  'lora_adapter_path': None,
[36m(TaskRunner pid=2594856)[0m                                  'lora_alpha': 16,
[36m(TaskRunner pid=2594856)[0m                                  'lora_rank': 0,
[36m(TaskRunner pid=2594856)[0m                                  'override_config': {'model_config': {},
[36m(TaskRunner pid=2594856)[0m                                                      'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=2594856)[0m                                  'path': '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge',
[36m(TaskRunner pid=2594856)[0m                                  'target_modules': 'all-linear',
[36m(TaskRunner pid=2594856)[0m                                  'tiled_mlp': {'enabled': False,
[36m(TaskRunner pid=2594856)[0m                                                'num_shards': 4},
[36m(TaskRunner pid=2594856)[0m                                  'tokenizer_path': None,
[36m(TaskRunner pid=2594856)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_liger': False,
[36m(TaskRunner pid=2594856)[0m                                  'use_remove_padding': True,
[36m(TaskRunner pid=2594856)[0m                                  'use_shm': False},
[36m(TaskRunner pid=2594856)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=2594856)[0m                        'ref': {'_target_': 'verl.workers.config.VeOmniActorConfig',
[36m(TaskRunner pid=2594856)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2594856)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2594856)[0m                                'log_prob_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=2594856)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=2594856)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=2594856)[0m                                             'enable': False,
[36m(TaskRunner pid=2594856)[0m                                             'ranks': [],
[36m(TaskRunner pid=2594856)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                                             'tool': None,
[36m(TaskRunner pid=2594856)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=2594856)[0m                                                                     'contents': [],
[36m(TaskRunner pid=2594856)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                                     'level': 'level0'},
[36m(TaskRunner pid=2594856)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=2594856)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=2594856)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=2594856)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2594856)[0m                                'rollout_n': 8,
[36m(TaskRunner pid=2594856)[0m                                'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=2594856)[0m                                                  'mode': 'disabled',
[36m(TaskRunner pid=2594856)[0m                                                  'record_file': None,
[36m(TaskRunner pid=2594856)[0m                                                  'replay_file': None},
[36m(TaskRunner pid=2594856)[0m                                'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                                'use_torch_compile': False,
[36m(TaskRunner pid=2594856)[0m                                'veomni': {'_target_': 'verl.workers.config.VeOmniEngineConfig',
[36m(TaskRunner pid=2594856)[0m                                           'activation_gpu_limit': 0.0,
[36m(TaskRunner pid=2594856)[0m                                           'attn_implementation': 'flash_attention_2',
[36m(TaskRunner pid=2594856)[0m                                           'ckpt_manager': 'dcp',
[36m(TaskRunner pid=2594856)[0m                                           'context_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'data_parallel_mode': 'fsdp2',
[36m(TaskRunner pid=2594856)[0m                                           'data_parallel_replicate_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'data_parallel_shard_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'data_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                           'enable_fsdp_offload': False,
[36m(TaskRunner pid=2594856)[0m                                           'enable_full_shard': True,
[36m(TaskRunner pid=2594856)[0m                                           'enable_reentrant': False,
[36m(TaskRunner pid=2594856)[0m                                           'expert_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                           'force_use_huggingface': False,
[36m(TaskRunner pid=2594856)[0m                                           'forward_only': True,
[36m(TaskRunner pid=2594856)[0m                                           'forward_prefetch': True,
[36m(TaskRunner pid=2594856)[0m                                           'full_determinism': False,
[36m(TaskRunner pid=2594856)[0m                                           'init_device': 'meta',
[36m(TaskRunner pid=2594856)[0m                                           'mixed_precision': True,
[36m(TaskRunner pid=2594856)[0m                                           'moe_implementation': 'fused',
[36m(TaskRunner pid=2594856)[0m                                           'optimizer_offload': False,
[36m(TaskRunner pid=2594856)[0m                                           'param_offload': True,
[36m(TaskRunner pid=2594856)[0m                                           'pipeline_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'seed': 42,
[36m(TaskRunner pid=2594856)[0m                                           'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                                           'tensor_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'ulysses_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                           'use_torch_compile': False}},
[36m(TaskRunner pid=2594856)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=2594856)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=2594856)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=2594856)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=2594856)[0m                                                                      'name': None,
[36m(TaskRunner pid=2594856)[0m                                                                      'path': None},
[36m(TaskRunner pid=2594856)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=2594856)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=2594856)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=2594856)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=2594856)[0m                                    'data_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=2594856)[0m                                    'do_sample': True,
[36m(TaskRunner pid=2594856)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=2594856)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=2594856)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=2594856)[0m                                    'enable_rollout_routing_replay': False,
[36m(TaskRunner pid=2594856)[0m                                    'enforce_eager': False,
[36m(TaskRunner pid=2594856)[0m                                    'engine_kwargs': {'sglang': {},
[36m(TaskRunner pid=2594856)[0m                                                      'vllm': {'disable_mm_preprocessor_cache': True}},
[36m(TaskRunner pid=2594856)[0m                                    'expert_parallel_size': 16,
[36m(TaskRunner pid=2594856)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=2594856)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=2594856)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=2594856)[0m                                    'layer_name_map': {'gate_proj_layer_name': 'gate_up',
[36m(TaskRunner pid=2594856)[0m                                                       'qkv_layer_name': 'qkv'},
[36m(TaskRunner pid=2594856)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=2594856)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2594856)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2594856)[0m                                    'log_prob_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=2594856)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=2594856)[0m                                    'logprobs_mode': 'processed_logprobs',
[36m(TaskRunner pid=2594856)[0m                                    'max_model_len': 10240,
[36m(TaskRunner pid=2594856)[0m                                    'max_num_batched_tokens': 1024,
[36m(TaskRunner pid=2594856)[0m                                    'max_num_seqs': 128,
[36m(TaskRunner pid=2594856)[0m                                    'mode': 'async',
[36m(TaskRunner pid=2594856)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=2594856)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=2594856)[0m                                                   'enable': False,
[36m(TaskRunner pid=2594856)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=2594856)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=2594856)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=2594856)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=2594856)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=2594856)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=2594856)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=2594856)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=2594856)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=2594856)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=2594856)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=2594856)[0m                                    'n': 8,
[36m(TaskRunner pid=2594856)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=2594856)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=2594856)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=2594856)[0m                                                 'enable': False,
[36m(TaskRunner pid=2594856)[0m                                                 'ranks': [],
[36m(TaskRunner pid=2594856)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                                                 'tool': None,
[36m(TaskRunner pid=2594856)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=2594856)[0m                                                                         'contents': [],
[36m(TaskRunner pid=2594856)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                                         'level': 'level0'},
[36m(TaskRunner pid=2594856)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=2594856)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=2594856)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=2594856)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2594856)[0m                                    'prometheus': {'_target_': 'verl.workers.config.PrometheusConfig',
[36m(TaskRunner pid=2594856)[0m                                                   'enable': False,
[36m(TaskRunner pid=2594856)[0m                                                   'file': '/tmp/ray/session_latest/metrics/prometheus/prometheus.yml',
[36m(TaskRunner pid=2594856)[0m                                                   'port': 9090,
[36m(TaskRunner pid=2594856)[0m                                                   'served_model_name': '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge'},
[36m(TaskRunner pid=2594856)[0m                                    'prompt_length': 2048,
[36m(TaskRunner pid=2594856)[0m                                    'quantization': None,
[36m(TaskRunner pid=2594856)[0m                                    'quantization_config_file': None,
[36m(TaskRunner pid=2594856)[0m                                    'response_length': 8192,
[36m(TaskRunner pid=2594856)[0m                                    'scheduling_policy': 'fcfs',
[36m(TaskRunner pid=2594856)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=2594856)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=2594856)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=2594856)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=2594856)[0m                                    'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                                    'top_k': -1,
[36m(TaskRunner pid=2594856)[0m                                    'top_p': 1,
[36m(TaskRunner pid=2594856)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=2594856)[0m                                              'backend': None,
[36m(TaskRunner pid=2594856)[0m                                              'max_samples_per_step_per_worker': None,
[36m(TaskRunner pid=2594856)[0m                                              'token2text': False},
[36m(TaskRunner pid=2594856)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=2594856)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=2594856)[0m                                                   'do_sample': False,
[36m(TaskRunner pid=2594856)[0m                                                   'n': 1,
[36m(TaskRunner pid=2594856)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=2594856)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=2594856)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=2594856)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=2594856)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=2594856)[0m                'gamma': 1.0,
[36m(TaskRunner pid=2594856)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=2594856)[0m                            'horizon': 10000,
[36m(TaskRunner pid=2594856)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=2594856)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=2594856)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=2594856)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=2594856)[0m                'lam': 1.0,
[36m(TaskRunner pid=2594856)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=2594856)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=2594856)[0m                'rollout_correction': {'bypass_mode': False,
[36m(TaskRunner pid=2594856)[0m                                       'loss_type': 'ppo_clip',
[36m(TaskRunner pid=2594856)[0m                                       'rollout_is': None,
[36m(TaskRunner pid=2594856)[0m                                       'rollout_is_batch_normalize': False,
[36m(TaskRunner pid=2594856)[0m                                       'rollout_is_threshold': 2.0,
[36m(TaskRunner pid=2594856)[0m                                       'rollout_rs': None,
[36m(TaskRunner pid=2594856)[0m                                       'rollout_rs_threshold': None},
[36m(TaskRunner pid=2594856)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=2594856)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=2594856)[0m  'critic': {'_target_': 'verl.workers.config.VeOmniCriticConfig',
[36m(TaskRunner pid=2594856)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=2594856)[0m                            'async_save': False,
[36m(TaskRunner pid=2594856)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=2594856)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=2594856)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=2594856)[0m             'data_loader_seed': 42,
[36m(TaskRunner pid=2594856)[0m             'enable': None,
[36m(TaskRunner pid=2594856)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2594856)[0m             'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=2594856)[0m             'model': {'_target_': 'verl.trainer.config.BaseModelConfig',
[36m(TaskRunner pid=2594856)[0m                       'external_lib': None,
[36m(TaskRunner pid=2594856)[0m                       'override_config': {},
[36m(TaskRunner pid=2594856)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=2594856)[0m                       'tokenizer_path': '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge',
[36m(TaskRunner pid=2594856)[0m                       'trust_remote_code': False},
[36m(TaskRunner pid=2594856)[0m             'optim': {'_target_': 'verl.workers.config.VeOmniOptimizerConfig',
[36m(TaskRunner pid=2594856)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=2594856)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=2594856)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=2594856)[0m                       'lr_decay_ratio': 1.0,
[36m(TaskRunner pid=2594856)[0m                       'lr_min': 0.0,
[36m(TaskRunner pid=2594856)[0m                       'lr_scheduler_type': 'cosine',
[36m(TaskRunner pid=2594856)[0m                       'lr_start': 0.0,
[36m(TaskRunner pid=2594856)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=2594856)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2594856)[0m                       'optimizer': 'adamw',
[36m(TaskRunner pid=2594856)[0m                       'override_optimizer_config': {},
[36m(TaskRunner pid=2594856)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=2594856)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=2594856)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=2594856)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2594856)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2594856)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2594856)[0m             'ppo_mini_batch_size': 4,
[36m(TaskRunner pid=2594856)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                          'all_ranks': False,
[36m(TaskRunner pid=2594856)[0m                          'enable': False,
[36m(TaskRunner pid=2594856)[0m                          'ranks': [],
[36m(TaskRunner pid=2594856)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                          'tool': None,
[36m(TaskRunner pid=2594856)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                  'analysis': True,
[36m(TaskRunner pid=2594856)[0m                                                  'contents': [],
[36m(TaskRunner pid=2594856)[0m                                                  'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                  'level': 'level0'},
[36m(TaskRunner pid=2594856)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                   'discrete': False},
[36m(TaskRunner pid=2594856)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                    'step_end': None,
[36m(TaskRunner pid=2594856)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=2594856)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2594856)[0m             'rollout_n': 8,
[36m(TaskRunner pid=2594856)[0m             'shuffle': False,
[36m(TaskRunner pid=2594856)[0m             'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m             'use_dynamic_bsz': False,
[36m(TaskRunner pid=2594856)[0m             'veomni': {'_target_': 'verl.workers.config.VeOmniEngineConfig',
[36m(TaskRunner pid=2594856)[0m                        'activation_gpu_limit': 0.0,
[36m(TaskRunner pid=2594856)[0m                        'attn_implementation': 'flash_attention_2',
[36m(TaskRunner pid=2594856)[0m                        'ckpt_manager': 'dcp',
[36m(TaskRunner pid=2594856)[0m                        'context_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'data_parallel_mode': 'fsdp2',
[36m(TaskRunner pid=2594856)[0m                        'data_parallel_replicate_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'data_parallel_shard_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'data_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'enable_fsdp_offload': False,
[36m(TaskRunner pid=2594856)[0m                        'enable_full_shard': True,
[36m(TaskRunner pid=2594856)[0m                        'enable_reentrant': False,
[36m(TaskRunner pid=2594856)[0m                        'expert_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'force_use_huggingface': False,
[36m(TaskRunner pid=2594856)[0m                        'forward_only': False,
[36m(TaskRunner pid=2594856)[0m                        'forward_prefetch': True,
[36m(TaskRunner pid=2594856)[0m                        'full_determinism': False,
[36m(TaskRunner pid=2594856)[0m                        'init_device': 'meta',
[36m(TaskRunner pid=2594856)[0m                        'mixed_precision': True,
[36m(TaskRunner pid=2594856)[0m                        'moe_implementation': 'fused',
[36m(TaskRunner pid=2594856)[0m                        'optimizer_offload': False,
[36m(TaskRunner pid=2594856)[0m                        'param_offload': False,
[36m(TaskRunner pid=2594856)[0m                        'pipeline_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'seed': 42,
[36m(TaskRunner pid=2594856)[0m                        'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                        'tensor_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'ulysses_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                        'use_torch_compile': False}},
[36m(TaskRunner pid=2594856)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=2594856)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=2594856)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=2594856)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=2594856)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=2594856)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=2594856)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=2594856)[0m           'image_key': 'images',
[36m(TaskRunner pid=2594856)[0m           'image_patch_size': 14,
[36m(TaskRunner pid=2594856)[0m           'max_prompt_length': 2048,
[36m(TaskRunner pid=2594856)[0m           'max_response_length': 8192,
[36m(TaskRunner pid=2594856)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=2594856)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=2594856)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=2594856)[0m           'return_raw_chat': True,
[36m(TaskRunner pid=2594856)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=2594856)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=2594856)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=2594856)[0m           'seed': None,
[36m(TaskRunner pid=2594856)[0m           'shuffle': True,
[36m(TaskRunner pid=2594856)[0m           'tokenizer': None,
[36m(TaskRunner pid=2594856)[0m           'tool_config_path': None,
[36m(TaskRunner pid=2594856)[0m           'train_batch_size': 16,
[36m(TaskRunner pid=2594856)[0m           'train_files': '/mnt/share/nurxat/gptoss/data/gsm8k/train.parquet',
[36m(TaskRunner pid=2594856)[0m           'train_max_samples': -1,
[36m(TaskRunner pid=2594856)[0m           'truncation': 'error',
[36m(TaskRunner pid=2594856)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=2594856)[0m           'use_shm': False,
[36m(TaskRunner pid=2594856)[0m           'val_batch_size': None,
[36m(TaskRunner pid=2594856)[0m           'val_files': '/mnt/share/nurxat/gptoss/data/gsm8k/test.parquet',
[36m(TaskRunner pid=2594856)[0m           'val_max_samples': -1,
[36m(TaskRunner pid=2594856)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=2594856)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=2594856)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                      'global_tool_config': {'nsys': {'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=2594856)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=2594856)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=2594856)[0m                                                      'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=2594856)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=2594856)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=2594856)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=2594856)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=2594856)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=2594856)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=2594856)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=2594856)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=2594856)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=2594856)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=2594856)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                      'steps': None,
[36m(TaskRunner pid=2594856)[0m                      'tool': None},
[36m(TaskRunner pid=2594856)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=2594856)[0m  'reward_manager': {'_target_': 'verl.trainer.config.config.RewardManagerConfig',
[36m(TaskRunner pid=2594856)[0m                     'module': {'_target_': 'verl.trainer.config.config.ModuleConfig',
[36m(TaskRunner pid=2594856)[0m                                'name': 'custom_reward_manager',
[36m(TaskRunner pid=2594856)[0m                                'path': None},
[36m(TaskRunner pid=2594856)[0m                     'name': 'naive',
[36m(TaskRunner pid=2594856)[0m                     'source': 'register'},
[36m(TaskRunner pid=2594856)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=2594856)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=2594856)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2594856)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=2594856)[0m                   'max_length': None,
[36m(TaskRunner pid=2594856)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=2594856)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2594856)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=2594856)[0m                             'input_tokenizer': '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge',
[36m(TaskRunner pid=2594856)[0m                             'override_config': {},
[36m(TaskRunner pid=2594856)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=2594856)[0m                             'trust_remote_code': False},
[36m(TaskRunner pid=2594856)[0m                   'n_gpus_per_node': 8,
[36m(TaskRunner pid=2594856)[0m                   'nnodes': 0,
[36m(TaskRunner pid=2594856)[0m                   'num_workers': 1,
[36m(TaskRunner pid=2594856)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2594856)[0m                                'all_ranks': False,
[36m(TaskRunner pid=2594856)[0m                                'enable': False,
[36m(TaskRunner pid=2594856)[0m                                'ranks': [],
[36m(TaskRunner pid=2594856)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=2594856)[0m                                'tool': None,
[36m(TaskRunner pid=2594856)[0m                                'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                        'analysis': True,
[36m(TaskRunner pid=2594856)[0m                                                        'contents': [],
[36m(TaskRunner pid=2594856)[0m                                                        'discrete': False,
[36m(TaskRunner pid=2594856)[0m                                                        'level': 'level0'},
[36m(TaskRunner pid=2594856)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                         'discrete': False},
[36m(TaskRunner pid=2594856)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                          'step_end': None,
[36m(TaskRunner pid=2594856)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=2594856)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2594856)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=2594856)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2594856)[0m                   'reward_loop_class_name': None,
[36m(TaskRunner pid=2594856)[0m                   'reward_loop_module_path': None,
[36m(TaskRunner pid=2594856)[0m                   'reward_loop_source': 'register',
[36m(TaskRunner pid=2594856)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=2594856)[0m                   'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=2594856)[0m                               'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=2594856)[0m                               'data_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                               'disable_log_stats': True,
[36m(TaskRunner pid=2594856)[0m                               'dtype': 'bfloat16',
[36m(TaskRunner pid=2594856)[0m                               'enable_chunked_prefill': True,
[36m(TaskRunner pid=2594856)[0m                               'enable_prefix_caching': True,
[36m(TaskRunner pid=2594856)[0m                               'enforce_eager': True,
[36m(TaskRunner pid=2594856)[0m                               'engine_kwargs': {},
[36m(TaskRunner pid=2594856)[0m                               'expert_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                               'free_cache_engine': True,
[36m(TaskRunner pid=2594856)[0m                               'gpu_memory_utilization': 0.5,
[36m(TaskRunner pid=2594856)[0m                               'limit_images': None,
[36m(TaskRunner pid=2594856)[0m                               'load_format': 'auto',
[36m(TaskRunner pid=2594856)[0m                               'max_model_len': None,
[36m(TaskRunner pid=2594856)[0m                               'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=2594856)[0m                               'max_num_seqs': 1024,
[36m(TaskRunner pid=2594856)[0m                               'name': '???',
[36m(TaskRunner pid=2594856)[0m                               'prompt_length': 2048,
[36m(TaskRunner pid=2594856)[0m                               'response_length': 2048,
[36m(TaskRunner pid=2594856)[0m                               'skip_tokenizer_init': False,
[36m(TaskRunner pid=2594856)[0m                               'tensor_model_parallel_size': 2},
[36m(TaskRunner pid=2594856)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=2594856)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=2594856)[0m                                      'url': None},
[36m(TaskRunner pid=2594856)[0m                   'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                   'use_dynamic_bsz': False,
[36m(TaskRunner pid=2594856)[0m                   'use_reward_loop': True,
[36m(TaskRunner pid=2594856)[0m                   'veomni': {'_target_': 'verl.workers.config.VeOmniEngineConfig',
[36m(TaskRunner pid=2594856)[0m                              'activation_gpu_limit': 0.0,
[36m(TaskRunner pid=2594856)[0m                              'attn_implementation': 'flash_attention_2',
[36m(TaskRunner pid=2594856)[0m                              'ckpt_manager': 'dcp',
[36m(TaskRunner pid=2594856)[0m                              'context_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'data_parallel_mode': 'fsdp2',
[36m(TaskRunner pid=2594856)[0m                              'data_parallel_replicate_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'data_parallel_shard_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'data_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'enable_fsdp_offload': False,
[36m(TaskRunner pid=2594856)[0m                              'enable_full_shard': True,
[36m(TaskRunner pid=2594856)[0m                              'enable_reentrant': False,[36m(TaskRunner pid=2594856)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/trainer/main_ppo.py:312: UserWarning: Disabled critic as algorithm.adv_estimator != gae. If it is not intended, please set critic.enable=True
[36m(TaskRunner pid=2594856)[0m   use_critic=need_critic(config),
[36m(TaskRunner pid=2594856)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.
[36m(TaskRunner pid=2594856)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(TaskRunner pid=2594856)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(TaskRunner pid=2594856)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(TaskRunner pid=2594856)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(TaskRunner pid=2594856)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)
[36m(TaskRunner pid=2594856)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/trainer/ppo/ray_trainer.py:347: UserWarning: Disabled critic as algorithm.adv_estimator != gae. If it is not intended, please set critic.enable=True
[36m(TaskRunner pid=2594856)[0m   self.use_critic = need_critic(self.config)
[36m(WorkerDict pid=2597107)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(WorkerDict pid=2597107)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(WorkerDict pid=2597107)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(WorkerDict pid=2597107)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)
[36m(WorkerDict pid=2597107)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.
[36m(WorkerDict pid=2597107)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(WorkerDict pid=2597107)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:345: ImportWarning: 
[36m(WorkerDict pid=2597107)[0m     *************************************************************************************************************
[36m(WorkerDict pid=2597107)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(WorkerDict pid=2597107)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(WorkerDict pid=2597107)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(WorkerDict pid=2597107)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(WorkerDict pid=2597107)[0m     The device parameters have been replaced with npu in the function below:
[36m(WorkerDict pid=2597107)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(WorkerDict pid=2597107)[0m     *************************************************************************************************************
[36m(WorkerDict pid=2597107)[0m     
[36m(WorkerDict pid=2597107)[0m   warnings.warn(msg, ImportWarning)
[36m(WorkerDict pid=2597107)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(WorkerDict pid=2597107)[0m   warnings.warn(msg, RuntimeWarning)
[36m(WorkerDict pid=2597107)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(WorkerDict pid=2597107)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(WorkerDict pid=2597107)[0m   import pkg_resources

[36m(TaskRunner pid=2594856)[0m                              'expert_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'force_use_huggingface': False,
[36m(TaskRunner pid=2594856)[0m                              'forward_only': False,
[36m(TaskRunner pid=2594856)[0m                              'forward_prefetch': True,
[36m(TaskRunner pid=2594856)[0m                              'full_determinism': False,
[36m(TaskRunner pid=2594856)[0m                              'init_device': 'meta',
[36m(TaskRunner pid=2594856)[0m                              'mixed_precision': True,
[36m(TaskRunner pid=2594856)[0m                              'moe_implementation': 'eager',
[36m(TaskRunner pid=2594856)[0m                              'pipeline_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'seed': 42,
[36m(TaskRunner pid=2594856)[0m                              'strategy': 'veomni',
[36m(TaskRunner pid=2594856)[0m                              'tensor_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'ulysses_parallel_size': 1,
[36m(TaskRunner pid=2594856)[0m                              'use_torch_compile': False}},
[36m(TaskRunner pid=2594856)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=2594856)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=2594856)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=2594856)[0m              'default_local_dir': 'checkpoints/verl_grpo_example_geo3k/qwen2_5_vl_3b_function_rm',
[36m(TaskRunner pid=2594856)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=2594856)[0m              'device': 'npu',
[36m(TaskRunner pid=2594856)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=2594856)[0m              'experiment_name': 'qwen2_5_vl_3b_function_rm',
[36m(TaskRunner pid=2594856)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=2594856)[0m              'logger': 'console',
[36m(TaskRunner pid=2594856)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=2594856)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=2594856)[0m              'n_gpus_per_node': 16,
[36m(TaskRunner pid=2594856)[0m              'nnodes': 1,
[36m(TaskRunner pid=2594856)[0m              'project_name': 'verl_grpo_example_geo3k',
[36m(TaskRunner pid=2594856)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=2594856)[0m              'resume_from_path': None,
[36m(TaskRunner pid=2594856)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=2594856)[0m              'rollout_data_dir': None,
[36m(TaskRunner pid=2594856)[0m              'save_freq': -1,
[36m(TaskRunner pid=2594856)[0m              'test_freq': -1,
[36m(TaskRunner pid=2594856)[0m              'total_epochs': 15,
[36m(TaskRunner pid=2594856)[0m              'total_training_steps': None,
[36m(TaskRunner pid=2594856)[0m              'use_legacy_worker_impl': 'disable',
[36m(TaskRunner pid=2594856)[0m              'val_before_train': True},
[36m(TaskRunner pid=2594856)[0m  'transfer_queue': {'enable': False}}
[36m(TaskRunner pid=2594856)[0m Using new worker implementation
[36m(TaskRunner pid=2594856)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=2594856)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=2594856)[0m dataset len: 7473
[36m(TaskRunner pid=2594856)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=2594856)[0m dataset len: 1319
[36m(TaskRunner pid=2594856)[0m Size of train dataloader: 467, Size of val dataloader: 1
[36m(TaskRunner pid=2594856)[0m Total training steps: 7005
[36m(TaskRunner pid=2594856)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(TaskRunner pid=2594856)[0m bind role actor_rollout_ref method chat_completion to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=2594856)[0m bind role actor_rollout_ref method generate to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=2594856)[0m bind role actor_rollout_ref method get_zeromq_address to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=2594856)[0m bind role actor_rollout_ref method sleep to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=2594856)[0m bind role actor_rollout_ref method wake_up to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:58:14 >> âœ… VeOmni ops patch applied.
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:58:14 >> âŒ veomni_patch is not available
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:58:14 >> 
[36m(WorkerDict pid=2597107)[0m ========== Environment Variables ==========
[36m(WorkerDict pid=2597107)[0m MODELING_BACKEND=veomni (source=default)
[36m(WorkerDict pid=2597107)[0m USE_GROUP_GEMM=1 (source=default)
[36m(WorkerDict pid=2597107)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(WorkerDict pid=2597107)[0m ===========================================
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:58:14 >> 
[36m(WorkerDict pid=2597107)[0m =========== OPS ============
[36m(WorkerDict pid=2597107)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(WorkerDict pid=2597107)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(WorkerDict pid=2597107)[0m _cross_entropy = eager_cross_entropy
[36m(WorkerDict pid=2597107)[0m ==============================
[36m(WorkerDict pid=2597107)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:58:14,635 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:493] 02/03/2026 07:58:17 >> Initializing parallel state... dp_size 16, dp_replicate_size 1, dp_shard_size 16,tp_size 1, pp_size 1, ep_size 16, cp_size 1, ulysses_size 1
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:564] 02/03/2026 07:58:17 >> Device mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('dp_shard',))
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:565] 02/03/2026 07:58:17 >> EP FSDP device mesh: DeviceMesh('npu', [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], mesh_dim_names=('ep', 'ep_fsdp'))
[36m(WorkerDict pid=2597109)[0m    -----PP (æµæ°´çº¿å¹¶è¡Œ)å¤§å°: 1
[36m(WorkerDict pid=2597109)[0m    DP (æ•°æ®å¹¶è¡Œ)å¤§å°: 16
[36m(WorkerDict pid=2597109)[0m    CP (ä¸Šä¸‹æ–‡å¹¶è¡Œ)å¤§å°: 1
[36m(WorkerDict pid=2597109)[0m    Ulysseså¤§å°: 1
[36m(WorkerDict pid=2597109)[0m    TP (å¼ é‡å¹¶è¡Œ)å¤§å°: 1
[36m(WorkerDict pid=2597109)[0m    ä¹˜ç§¯: 16
[36m(WorkerDict pid=2597109)[0m    World Size (æ€»GPUæ•°): 16
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:59] 02/03/2026 07:58:17 >> [CONFIG] Loading qwen3_moe from Huggingface and replaced with customized config.
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:93] 02/03/2026 07:58:17 >> Moe implementation: fused
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:199] 02/03/2026 07:58:17 >> Loading model from customized modeling.
[36m(WorkerDict pid=2597109)[0m init_device: meta[36m(WorkerDict pid=2597109)[0m `torch_dtype` is deprecated! Use `dtype` instead!

[36m(WorkerDict pid=2597109)[0m empty_init: True
[36m(WorkerDict pid=2597109)[0m weights_path: /mnt/share/w00914260/model/Qwen3-30B-MoE-merge
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:119] 02/03/2026 07:58:17 >> We override the modelâ€™s forward method on NPU devices to ensure that the FA kwargs are on CPU, since the npu_fused_attention requires cpu FA kwargs
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:460] 02/03/2026 07:58:17 >> Enable gradient checkpointing.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:480] 02/03/2026 07:58:17 >> Apply data parallel to the model: fsdp2.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:256] 02/03/2026 07:58:17 >> target classes to shard: {'Qwen3MoeDecoderLayer'}
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.0.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.0.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.0.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.1.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.1.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.1.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597110)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:274] 02/03/2026 07:58:17 >> Applied EP: expert tensors sliced along expert dimension (EP mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('ep',)))
[36m(WorkerDict pid=2597110)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:275] 02/03/2026 07:58:17 >> Experts Map: {'model.layers.0.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.1.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.2.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.3.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.4.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.5.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.6.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.7.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.8.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.9.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.10.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.11.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.12.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.13.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.14.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.15.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.16.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.17.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.18.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.19.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.20.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.21.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.22.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.23.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.24.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.25.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.26.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.27.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.28.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.29.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.30.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.31.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.32.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.33.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.34.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.35.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.36.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.37.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.38.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.39.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.40.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.41.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.42.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.43.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.44.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.45.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.46.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m ), 'model.layers.47.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )}
[36m(WorkerDict pid=2597110)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:296] 02/03/2026 07:58:17 >> layer pairs: [('model.layers.0', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.1', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.2', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.3', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.4', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.5', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.6', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.7', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.8', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.9', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.10', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.11', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.12', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.13', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.14', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.15', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.16', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.17', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.18', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.19', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.20', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.21', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.22', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.23', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.24', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.25', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.26', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.27', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.28', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.29', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.30', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.31', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.32', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.33', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.34', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.35', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.36', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.37', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.38', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.39', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.40', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.41', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.42', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.43', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[36m(WorkerDict pid=2597109)[0m Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]
[36m(WorkerDict pid=2597121)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.[32m [repeated 30x across cluster][0m
[36m(WorkerDict pid=2597121)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 15x across cluster][0m

[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.44', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597110)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597110)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m     )
[36m(WorkerDict pid=2597110)[0m   )
[36m(WorkerDict pid=2597110)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597110)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597110)[0m )), ('model.layers.45', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597110)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597110)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597110)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597110)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597110)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597110)[0m ))]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597114)[0m )]
[36m(WorkerDict pid=2597113)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:416] 02/03/2026 07:58:21 >> starting to load model weights...
[36m(WorkerDict pid=2597113)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:421] 02/03/2026 07:58:21 >> Every rank would read weights from disk and expect this to be slow!
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:58:15 >> âœ… VeOmni ops patch applied.[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:58:15 >> âŒ veomni_patch is not available[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:18 >> layer_fqn='model.layers.47', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts([32m [repeated 750x across cluster][0m
[36m(WorkerDict pid=2597121)[0m ========== Environment Variables ==========[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m MODELING_BACKEND=veomni (source=default)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m USE_LIGER_KERNEL=1 (source=default)[32m [repeated 30x across cluster][0m
[36m(WorkerDict pid=2597121)[0m ===========================================[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m =========== OPS ============[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m _fused_moe_forward = npu_fused_moe_forward[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m _flash_attention_forward = transformers_flash_attention_forward[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m _cross_entropy = eager_cross_entropy[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m ==============================[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [Rank 14 | Local Rank 0] 2026-02-03 07:58:16,247 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:493] 02/03/2026 07:58:17 >> Initializing parallel state... dp_size 16, dp_replicate_size 1, dp_shard_size 16,tp_size 1, pp_size 1, ep_size 16, cp_size 1, ulysses_size 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:564] 02/03/2026 07:58:17 >> Device mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('dp_shard',))[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:565] 02/03/2026 07:58:17 >> EP FSDP device mesh: DeviceMesh('npu', [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], mesh_dim_names=('ep', 'ep_fsdp'))[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    -----PP (æµæ°´çº¿å¹¶è¡Œ)å¤§å°: 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    DP (æ•°æ®å¹¶è¡Œ)å¤§å°: 16[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    CP (ä¸Šä¸‹æ–‡å¹¶è¡Œ)å¤§å°: 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    Ulysseså¤§å°: 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    TP (å¼ é‡å¹¶è¡Œ)å¤§å°: 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    ä¹˜ç§¯: 16[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m    World Size (æ€»GPUæ•°): 16[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:59] 02/03/2026 07:58:17 >> [CONFIG] Loading qwen3_moe from Huggingface and replaced with customized config.[32m [repeated 15x across cluster][0m[36m(WorkerDict pid=2597107)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: Cannot create tensor with interal format while allow_internel_format=False, tensor will be created with base format. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:335.)
[36m(WorkerDict pid=2597107)[0m   return fn(*args, **kwargs)
[36m(WorkerDict pid=2597107)[0m Loading checkpoint shards:   8%|â–Š         | 1/13 [00:04<00:54,  4.57s/it]
[36m(WorkerDict pid=2597107)[0m Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s][32m [repeated 15x across cluster][0m

[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:93] 02/03/2026 07:58:17 >> Moe implementation: fused[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:199] 02/03/2026 07:58:17 >> Loading model from customized modeling.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m init_device: meta[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m empty_init: True[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597122)[0m weights_path: /mnt/share/w00914260/model/Qwen3-30B-MoE-merge[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:119] 02/03/2026 07:58:17 >> We override the modelâ€™s forward method on NPU devices to ensure that the FA kwargs are on CPU, since the npu_fused_attention requires cpu FA kwargs[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:460] 02/03/2026 07:58:17 >> Enable gradient checkpointing.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:480] 02/03/2026 07:58:17 >> Apply data parallel to the model: fsdp2.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:256] 02/03/2026 07:58:17 >> target classes to shard: {'Qwen3MoeDecoderLayer'}[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:17 >> EP sharding: slicing param model.layers.47.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)][32m [repeated 2154x across cluster][0m
[36m(WorkerDict pid=2597117)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:274] 02/03/2026 07:58:17 >> Applied EP: expert tensors sliced along expert dimension (EP mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('ep',)))[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597117)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:275] 02/03/2026 07:58:17 >> Experts Map: {'model.layers.0.mlp.experts': Qwen3MoeExperts([32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597122)[0m       (act_fn): SiLUActivation()[32m [repeated 3462x across cluster][0m
[36m(WorkerDict pid=2597122)[0m ), FSDPQwen3MoeDecoderLayer([32m [repeated 2053x across cluster][0m
[36m(WorkerDict pid=2597117)[0m )}[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597117)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:296] 02/03/2026 07:58:17 >> layer pairs: [('model.layers.0', Qwen3MoeDecoderLayer([32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597122)[0m   (self_attn): Qwen3MoeAttention([32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 1394x across cluster][0m
[36m(WorkerDict pid=2597122)[0m   )[32m [repeated 4185x across cluster][0m
[36m(WorkerDict pid=2597122)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock([32m [repeated 1395x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)[32m [repeated 1395x across cluster][0m
[36m(WorkerDict pid=2597122)[0m     (experts): FSDPQwen3MoeExperts([32m [repeated 1395x across cluster][0m
[36m(WorkerDict pid=2597122)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 1395x across cluster][0m
[36m(WorkerDict pid=2597122)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 1395x across cluster][0m
[36m(WorkerDict pid=2597116)[0m )), ('model.layers.47', Qwen3MoeDecoderLayer([32m [repeated 660x across cluster][0m
[36m(WorkerDict pid=2597117)[0m ))][32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=2597122)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:18 >> setting grad divide factor for ep module to 16[32m [repeated 714x across cluster][0m
[36m(WorkerDict pid=2597122)[0m )][32m [repeated 712x across cluster][0m
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:26 >> Expert parameter model.layers.0.mlp.experts.down_proj: sliced torch.Size([128, 2048, 768]) -> torch.Size([8, 2048, 768]) for EP rank 2/16
[36m(WorkerDict pid=2597109)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:26 >> Expert parameter model.layers.0.mlp.experts.gate_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 2/16
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:416] 02/03/2026 07:58:23 >> starting to load model weights...[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:421] 02/03/2026 07:58:23 >> Every rank would read weights from disk and expect this to be slow![32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:23 >> layer_fqn='model.layers.47', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts([32m [repeated 48x across cluster][0m
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:460] 02/03/2026 07:58:23 >> Enable gradient checkpointing.
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:480] 02/03/2026 07:58:23 >> Apply data parallel to the model: fsdp2.
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:256] 02/03/2026 07:58:23 >> target classes to shard: {'Qwen3MoeDecoderLayer'}
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:23 >> EP sharding: slicing param model.layers.47.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)][32m [repeated 144x across cluster][0m
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:274] 02/03/2026 07:58:23 >> Applied EP: expert tensors sliced along expert dimension (EP mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('ep',)))
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:275] 02/03/2026 07:58:23 >> Experts Map: {'model.layers.0.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597107)[0m       (act_fn): SiLUActivation()[32m [repeated 240x across cluster][0m[36m(WorkerDict pid=2597112)[0m Loading checkpoint shards:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:11<00:00,  2.05it/s][32m [repeated 130x across cluster][0m
[36m(WorkerDict pid=2597112)[0m Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:11<00:00,  2.18it/s]
[36m(WorkerDict pid=2597112)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:11<00:00,  2.34it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:11<00:00,  1.10it/s]
[36m(WorkerDict pid=2597112)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.
[36m(WorkerDict pid=2597112)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)

[36m(WorkerDict pid=2597107)[0m ), FSDPQwen3MoeDecoderLayer([32m [repeated 143x across cluster][0m
[36m(WorkerDict pid=2597107)[0m )}
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:296] 02/03/2026 07:58:23 >> layer pairs: [('model.layers.0', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597107)[0m   (self_attn): Qwen3MoeAttention([32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m   )[32m [repeated 288x across cluster][0m
[36m(WorkerDict pid=2597107)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock([32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m     (experts): FSDPQwen3MoeExperts([32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 96x across cluster][0m
[36m(WorkerDict pid=2597107)[0m )), ('model.layers.47', Qwen3MoeDecoderLayer([32m [repeated 47x across cluster][0m
[36m(WorkerDict pid=2597107)[0m ))]
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:23 >> setting grad divide factor for ep module to 16[32m [repeated 48x across cluster][0m
[36m(WorkerDict pid=2597107)[0m )][32m [repeated 48x across cluster][0m
[36m(WorkerDict pid=2597116)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:31 >> Expert parameter model.layers.10.mlp.experts.gate_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 9/16[32m [repeated 786x across cluster][0m
[36m(WorkerDict pid=2597112)[0m [WARNING|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:483] 02/03/2026 07:58:35 >> Parallel state has already been initialized.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:59] 02/03/2026 07:58:35 >> [CONFIG] Loading qwen3_moe from Huggingface and replaced with customized config.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:93] 02/03/2026 07:58:35 >> Moe implementation: fused
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:199] 02/03/2026 07:58:35 >> Loading model from customized modeling.
[36m(WorkerDict pid=2597112)[0m init_device: meta
[36m(WorkerDict pid=2597112)[0m empty_init: True
[36m(WorkerDict pid=2597112)[0m weights_path: /mnt/share/w00914260/model/Qwen3-30B-MoE-merge
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:119] 02/03/2026 07:58:35 >> We override the modelâ€™s forward method on NPU devices to ensure that the FA kwargs are on CPU, since the npu_fused_attention requires cpu FA kwargs
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:460] 02/03/2026 07:58:35 >> Enable gradient checkpointing.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:480] 02/03/2026 07:58:35 >> Apply data parallel to the model: fsdp2.
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:256] 02/03/2026 07:58:35 >> target classes to shard: {'Qwen3MoeDecoderLayer'}
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.0.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.0.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.0.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.1.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.1.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.1.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.2.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.2.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.2.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.3.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.3.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.3.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.4.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.4.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.4.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.5.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.5.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.5.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.6.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.6.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.6.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.7.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.7.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.7.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.8.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.8.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.8.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.9.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.9.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.9.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.10.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.10.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.10.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.11.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.11.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.11.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.12.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.12.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.12.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.13.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.13.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.13.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.14.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.14.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.14.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.15.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.15.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.15.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.16.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.16.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.16.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.17.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.17.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.17.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.18.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.18.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.18.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.19.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.19.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.19.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.20.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.20.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.20.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.21.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.21.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.21.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.22.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.22.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.22.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.23.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.23.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.23.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.24.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.24.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.24.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.25.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.25.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.25.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.26.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.26.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.26.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.27.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.27.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.27.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.28.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.28.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.28.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.29.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.29.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.29.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.30.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.30.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.30.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.31.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.31.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.31.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.32.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.32.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.32.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.33.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.33.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.33.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.34.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.34.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.34.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.35.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.35.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.35.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.36.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.36.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.36.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.37.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.37.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.37.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.38.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.38.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.38.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.39.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.39.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.39.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.40.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.40.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.40.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.41.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.41.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.41.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.42.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.42.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.42.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.43.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.43.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.43.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.44.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.44.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.44.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.45.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.45.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.45.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.46.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.46.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.46.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.47.mlp.experts.gate_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.47.mlp.experts.up_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:35 >> EP sharding: slicing param model.layers.47.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:274] 02/03/2026 07:58:35 >> Applied EP: expert tensors sliced along expert dimension (EP mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('ep',)))
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:275] 02/03/2026 07:58:35 >> Experts Map: {'model.layers.0.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.1.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.2.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.3.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.4.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.5.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.6.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.7.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.8.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.9.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.10.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.11.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.12.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.13.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.14.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.15.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.16.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.17.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.18.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.19.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.20.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.21.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.22.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.23.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.24.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.25.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.26.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.27.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.28.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.29.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.30.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.31.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.32.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.33.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.34.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.35.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.36.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.37.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.38.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.39.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.40.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.41.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.42.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.43.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.44.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.45.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.46.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), 'model.layers.47.mlp.experts': Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )}
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:296] 02/03/2026 07:58:35 >> layer pairs: [('model.layers.0', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.1', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.2', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.3', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.4', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.5', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.6', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.7', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.8', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.9', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.10', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.11', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.12', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.13', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.14', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.15', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.16', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.17', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.18', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.19', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.20', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.21', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.22', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.23', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.24', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.25', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.26', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.27', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.28', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.29', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.30', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.31', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.32', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.33', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.34', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.35', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.36', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.37', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.38', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.39', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.40', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.41', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.42', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.43', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.44', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.45', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.46', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m )), ('model.layers.47', Qwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m ), Qwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ))]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.0', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.1', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.2', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.3', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.4', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.5', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.6', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.7', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.8', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.9', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.10', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.11', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.12', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.13', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.14', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.15', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.16', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.17', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.18', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.19', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m ), FSDPQwen3MoeDecoderLayer(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m       (act_fn): SiLUActivation()
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.20', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention([36m(WorkerDict pid=2597112)[0m Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]
[36m(WorkerDict pid=2597121)[0m Loading checkpoint shards:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:15<00:01,  1.15it/s][32m [repeated 45x across cluster][0m
[36m(WorkerDict pid=2597121)[0m Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:15<00:00,  1.26it/s][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:16<00:00,  1.40it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:16<00:00,  1.25s/it][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)[32m [repeated 15x across cluster][0m

[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:35 >> layer_fqn='model.layers.21', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m   (self_attn): Qwen3MoeAttention(
[36m(WorkerDict pid=2597112)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)
[36m(WorkerDict pid=2597112)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)
[36m(WorkerDict pid=2597112)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)
[36m(WorkerDict pid=2597112)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock(
[36m(WorkerDict pid=2597112)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)
[36m(WorkerDict pid=2597112)[0m     (experts): FSDPQwen3MoeExperts(
[36m(WorkerDict pid=2597112)[0m     )
[36m(WorkerDict pid=2597112)[0m   )
[36m(WorkerDict pid=2597112)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)
[36m(WorkerDict pid=2597112)[0m )]
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:35 >> setting grad divide factor for ep module to 16
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:416] 02/03/2026 07:58:35 >> starting to load model weights...
[36m(WorkerDict pid=2597112)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:421] 02/03/2026 07:58:35 >> Every rank would read weights from disk and expect this to be slow!
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:36 >> Expert parameter model.layers.37.mlp.experts.gate_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 14/16[32m [repeated 1424x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [WARNING|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_state.py:483] 02/03/2026 07:58:39 >> Parallel state has already been initialized.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:59] 02/03/2026 07:58:39 >> [CONFIG] Loading qwen3_moe from Huggingface and replaced with customized config.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:93] 02/03/2026 07:58:39 >> Moe implementation: fused[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/loader.py:199] 02/03/2026 07:58:39 >> Loading model from customized modeling.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m init_device: meta[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m empty_init: True[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m weights_path: /mnt/share/w00914260/model/Qwen3-30B-MoE-merge[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/models/auto.py:119] 02/03/2026 07:58:39 >> We override the modelâ€™s forward method on NPU devices to ensure that the FA kwargs are on CPU, since the npu_fused_attention requires cpu FA kwargs[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:460] 02/03/2026 07:58:39 >> Enable gradient checkpointing.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:480] 02/03/2026 07:58:39 >> Apply data parallel to the model: fsdp2.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:256] 02/03/2026 07:58:39 >> target classes to shard: {'Qwen3MoeDecoderLayer'}[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:65] 02/03/2026 07:58:40 >> EP sharding: slicing param model.layers.47.mlp.experts.down_proj along ep_mesh with placement [Shard(dim=0)][32m [repeated 2160x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:274] 02/03/2026 07:58:40 >> Applied EP: expert tensors sliced along expert dimension (EP mesh: DeviceMesh('npu', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], mesh_dim_names=('ep',)))[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:275] 02/03/2026 07:58:40 >> Experts Map: {'model.layers.0.mlp.experts': Qwen3MoeExperts([32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m       (act_fn): SiLUActivation()[32m [repeated 3656x across cluster][0m
[36m(WorkerDict pid=2597121)[0m ), FSDPQwen3MoeDecoderLayer([32m [repeated 2173x across cluster][0m
[36m(WorkerDict pid=2597121)[0m )}[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:296] 02/03/2026 07:58:40 >> layer pairs: [('model.layers.0', Qwen3MoeDecoderLayer([32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   (self_attn): Qwen3MoeAttention([32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (q_proj): Linear(in_features=2048, out_features=4096, bias=False)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (k_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (v_proj): Linear(in_features=2048, out_features=512, bias=False)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (o_proj): Linear(in_features=4096, out_features=2048, bias=False)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 1466x across cluster][0m[36m(WorkerDict pid=2597121)[0m Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597108)[0m Loading checkpoint shards:  15%|â–ˆâ–Œ        | 2/13 [00:06<00:30,  2.75s/it][32m [repeated 21x across cluster][0m
[36m(WorkerDict pid=2597108)[0m Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:10<00:00,  2.27it/s]
[36m(WorkerDict pid=2597108)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  2.34it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.18it/s]
[36m(WorkerDict pid=2597115)[0m Loading checkpoint shards:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:08<00:02,  1.45it/s][32m [repeated 145x across cluster][0m
[36m(WorkerDict pid=2597109)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
[36m(WorkerDict pid=2597109)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
[36m(WorkerDict pid=2597119)[0m Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:13<00:00,  1.82it/s][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597119)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  2.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  1.04s/it][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597119)[0m Loading checkpoint shards:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:12<00:01,  1.60it/s][32m [repeated 10x across cluster][0m
[36m(TaskRunner pid=2594856)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.
[36m(TaskRunner pid=2594856)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(pid=2601703)[0m   import pkg_resources
[36m(TaskRunner pid=2594856)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute[32m [repeated 16x across cluster][0m
[36m(TaskRunner pid=2594856)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute[32m [repeated 16x across cluster][0m
[36m(pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(pid=2601703)[0m     *************************************************************************************************************
[36m(pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(pid=2601703)[0m     *************************************************************************************************************
[36m(pid=2601703)[0m     
[36m(pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)

[36m(WorkerDict pid=2597121)[0m     (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   )[32m [repeated 4398x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   (mlp): Qwen3MoeSparseFusedMoeBlock([32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (gate): Linear(in_features=2048, out_features=128, bias=False)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m     (experts): FSDPQwen3MoeExperts([32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   (input_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m   (post_attention_layernorm): Qwen3MoeRMSNorm((2048,), eps=1e-06)[32m [repeated 1466x across cluster][0m
[36m(WorkerDict pid=2597121)[0m )), ('model.layers.47', Qwen3MoeDecoderLayer([32m [repeated 705x across cluster][0m
[36m(WorkerDict pid=2597121)[0m ))][32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:365] 02/03/2026 07:58:40 >> setting grad divide factor for ep module to 16[32m [repeated 745x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:383] 02/03/2026 07:58:40 >> layer_fqn='model.layers.47', layer_mod._fsdp_modules=[FSDPQwen3MoeExperts([32m [repeated 746x across cluster][0m
[36m(WorkerDict pid=2597121)[0m )][32m [repeated 746x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:416] 02/03/2026 07:58:40 >> starting to load model weights...[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597121)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/torch_parallelize.py:421] 02/03/2026 07:58:40 >> Every rank would read weights from disk and expect this to be slow![32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=2597107)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:43 >> Expert parameter model.layers.3.mlp.experts.up_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 0/16[32m [repeated 164x across cluster][0m
[36m(WorkerDict pid=2597108)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:48 >> Expert parameter model.layers.47.mlp.experts.up_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 1/16[32m [repeated 1753x across cluster][0m
[36m(WorkerDict pid=2597108)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/optim/optimizer.py:275] 02/03/2026 07:58:48 >> Building EP+FSDP2 optimizer
[36m(WorkerDict pid=2597108)[0m WARNING 02-03 07:58:55 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2597119)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/distributed/parallel_plan.py:157] 02/03/2026 07:58:51 >> Expert parameter model.layers.47.mlp.experts.up_proj: sliced torch.Size([128, 768, 2048]) -> torch.Size([8, 768, 2048]) for EP rank 12/16[32m [repeated 479x across cluster][0m
[36m(WorkerDict pid=2597119)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/optim/optimizer.py:275] 02/03/2026 07:58:51 >> Building EP+FSDP2 optimizer[32m [repeated 15x across cluster][0m
[36m(TaskRunner pid=2594856)[0m WARNING 02-03 07:58:58 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(pid=2601703)[0m WARNING 02-03 07:59:07 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")[32m [repeated 17x across cluster][0m
[36m(pid=2601703)[0m WARNING 02-03 07:59:07 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:08 >> âœ… VeOmni ops patch applied.
[36m(pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:08 >> âŒ veomni_patch is not available
[36m(pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:08 >> 
[36m(pid=2601703)[0m ========== Environment Variables ==========
[36m(pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(pid=2601703)[0m ===========================================
[36m(pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:08 >> 
[36m(pid=2601703)[0m =========== OPS ============
[36m(pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(pid=2601703)[0m ==============================
[36m(pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:08,453 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:09,687 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:231] => vLLMHttpServer, replica_rank: 0, master address: 141.61.29.117, master port: 36817, data parallel master port: 40111
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:09,695 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:271] => override_generation_config: {'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'repetition_penalty': 1.0, 'max_new_tokens': 8192}
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:09,695 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:273] => enable_sleep_mode: True
[36m(vLLMHttpServer pid=2601703)[0m ['serve',
[36m(vLLMHttpServer pid=2601703)[0m  '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge',
[36m(vLLMHttpServer pid=2601703)[0m  '--dtype',
[36m(vLLMHttpServer pid=2601703)[0m  'bfloat16',
[36m(vLLMHttpServer pid=2601703)[0m  '--load_format',
[36m(vLLMHttpServer pid=2601703)[0m  'dummy',
[36m(vLLMHttpServer pid=2601703)[0m  '--max_model_len',
[36m(vLLMHttpServer pid=2601703)[0m  '40960',
[36m(vLLMHttpServer pid=2601703)[0m  '--max_num_seqs',
[36m(vLLMHttpServer pid=2601703)[0m  '128',
[36m(vLLMHttpServer pid=2601703)[0m  '--enable_chunked_prefill',
[36m(vLLMHttpServer pid=2601703)[0m  '--max_num_batched_tokens',
[36m(vLLMHttpServer pid=2601703)[0m  '1024',
[36m(vLLMHttpServer pid=2601703)[0m  '--enable_prefix_caching',
[36m(vLLMHttpServer pid=2601703)[0m  '--enable_sleep_mode',
[36m(vLLMHttpServer pid=2601703)[0m  '--logprobs_mode',
[36m(vLLMHttpServer pid=2601703)[0m  'processed_logprobs',
[36m(vLLMHttpServer pid=2601703)[0m  '--disable_custom_all_reduce',
[36m(vLLMHttpServer pid=2601703)[0m  '--gpu_memory_utilization',
[36m(vLLMHttpServer pid=2601703)[0m  '0.6',
[36m(vLLMHttpServer pid=2601703)[0m  '--disable_log_stats',
[36m(vLLMHttpServer pid=2601703)[0m  '--tensor_parallel_size',
[36m(vLLMHttpServer pid=2601703)[0m  '1',
[36m(vLLMHttpServer pid=2601703)[0m  '--seed',
[36m(vLLMHttpServer pid=2601703)[0m  '0',
[36m(vLLMHttpServer pid=2601703)[0m  '--override_generation_config',
[36m(vLLMHttpServer pid=2601703)[0m  '{"temperature": 1.0, "top_k": -1, "top_p": 1, "repetition_penalty": 1.0, '
[36m(vLLMHttpServer pid=2601703)[0m  '"max_new_tokens": 8192}',
[36m(vLLMHttpServer pid=2601703)[0m  '--hf_overrides',
[36m(vLLMHttpServer pid=2601703)[0m  '{}',
[36m(vLLMHttpServer pid=2601703)[0m  '--scheduling_policy',[36m(vLLMHttpServer pid=2601703)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(vLLMHttpServer pid=2601703)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(vLLMHttpServer pid=2601703)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(vLLMHttpServer pid=2601703)[0m   import pkg_resources

[36m(vLLMHttpServer pid=2601703)[0m  'fcfs',
[36m(vLLMHttpServer pid=2601703)[0m  '--disable_mm_preprocessor_cache',
[36m(vLLMHttpServer pid=2601703)[0m  '--enable_expert_parallel',
[36m(vLLMHttpServer pid=2601703)[0m  '--data_parallel_size',
[36m(vLLMHttpServer pid=2601703)[0m  '16',
[36m(vLLMHttpServer pid=2601703)[0m  '--data_parallel_size_local',
[36m(vLLMHttpServer pid=2601703)[0m  '16',
[36m(vLLMHttpServer pid=2601703)[0m  '--data_parallel_start_rank',
[36m(vLLMHttpServer pid=2601703)[0m  '0',
[36m(vLLMHttpServer pid=2601703)[0m  '--data_parallel_address',
[36m(vLLMHttpServer pid=2601703)[0m  '141.61.29.117',
[36m(vLLMHttpServer pid=2601703)[0m  '--data_parallel_rpc_port',
[36m(vLLMHttpServer pid=2601703)[0m  '36817']
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [__init__.py:1750] argument 'disable_mm_preprocessor_cache' is deprecated
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:09,874 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:397] => replica_rank=0, node_rank=0, nnodes=1, get worker zmq addresses: ['ipc:///tmp/verl_vllm_zmq_2597107_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597108_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597109_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597110_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597111_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597112_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597113_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597114_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597115_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597116_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597117_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597118_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597119_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597120_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597121_root.ipc', 'ipc:///tmp/verl_vllm_zmq_2597122_root.ipc']
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [arg_utils.py:970] `--disable-mm-preprocessor-cache` is deprecated and will be removed in v0.13. Please use `--mm-processor-cache-gb 0` instead.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:09 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:10 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:20 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:21 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:22 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:22 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:22 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:22 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...

[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:22,556 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:22 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:22 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:22 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:22 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:22 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)

[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:23 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:23 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:23 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:23 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,248 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)

[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,312 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,322 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,334 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,367 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,384 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:23 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,504 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,507 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,549 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(vLLMHttpServer pid=2601703)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(vLLMHttpServer pid=2601703)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(vLLMHttpServer pid=2601703)[0m     The device parameters have been replaced with npu in the function below:
[36m(vLLMHttpServer pid=2601703)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(vLLMHttpServer pid=2601703)[0m     *************************************************************************************************************
[36m(vLLMHttpServer pid=2601703)[0m     
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, ImportWarning)
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(msg, RuntimeWarning)
[36m(vLLMHttpServer pid=2601703)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...

[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,666 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,731 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,751 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,788 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:23,802 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:23 >> âœ… VeOmni ops patch applied.
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:23 >> âŒ veomni_patch is not available
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m ========== Environment Variables ==========
[36m(vLLMHttpServer pid=2601703)[0m MODELING_BACKEND=veomni (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_GROUP_GEMM=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(vLLMHttpServer pid=2601703)[0m ===========================================
[36m(vLLMHttpServer pid=2601703)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:23 >> 
[36m(vLLMHttpServer pid=2601703)[0m =========== OPS ============
[36m(vLLMHttpServer pid=2601703)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(vLLMHttpServer pid=2601703)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(vLLMHttpServer pid=2601703)[0m _cross_entropy = eager_cross_entropy
[36m(vLLMHttpServer pid=2601703)[0m ==============================
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:24,259 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.[36m(WorkerDict pid=2597107)[0m [rank0]:[W203 07:59:36.256196690 compiler_depend.ts:117] Warning: Driver Version: ï¿½ï¿½Dï¿½ï¿½ is invalid or not supported yet. (function operator())
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 1/11 [00:00<00:03,  2.76it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 2/11 [00:00<00:02,  4.50it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 3/11 [00:00<00:01,  5.51it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [00:00<00:01,  6.08it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [00:00<00:00,  6.60it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [00:01<00:00,  7.05it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [00:01<00:00,  7.31it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [00:01<00:00,  7.49it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [00:01<00:00,  7.70it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:01<00:00,  7.26it/s]
[36m(WorkerDict pid=2597107)[0m Capturing ACL graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  7.64it/s]Capturing ACL graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  6.65it/s]
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.

[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen2_5OmniModel is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_omni_thinker:AscendQwen2_5OmniThinkerForConditionalGeneration.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v3_2:CustomDeepseekV3ForCausalLM.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:24 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2597109)[0m WARNING 02-03 07:59:30 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2597113)[0m WARNING 02-03 07:59:25 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development![32m [repeated 16x across cluster][0m
[36m(WorkerDict pid=2597116)[0m WARNING 02-03 07:59:25 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.[32m [repeated 112x across cluster][0m
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff0ba44b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff0dec4b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xfffefb984b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP4 pid=2602145)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xfffefe2d4ad0>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff2f2d4b30>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xfffef9e84b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff2ca18b30>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff055e8ad0>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff2ea98b30>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff32090b30>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff18f44b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff016a0ad0>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff26fe8b30>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xfffefbad4b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff2cc44ad0>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m   next(self.gen)
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/contextlib.py:144: ResourceWarning: Unclosed context <zmq.Context() at 0xffff15564b90>
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m   next(self.gen)

[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP8 pid=2602149)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP7 pid=2602148)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP3 pid=2602144)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP14 pid=2602155)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP15 pid=2602156)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP1 pid=2602142)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP5 pid=2602146)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP2 pid=2602143)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP10 pid=2602151)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP6 pid=2602147)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP11 pid=2602152)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP12 pid=2602153)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP0 pid=2602141)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP9 pid=2602150)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:45 [utils.py:453] Currently, communication is performed using FFTS+ method, which reduces the number of available streams and, as a result, limits the range of runtime shapes that can be handled. To both improve communication performance and increase the number of supported shapes, set HCCL_OP_EXPANSION_MODE=AIV.
[36m(vLLMHttpServer pid=2601703)[0m [1;36m(EngineCore_DP13 pid=2602154)[0;0m WARNING 02-03 07:59:45 [platform.py:275] If chunked prefill or prefix caching is enabled, block size must be set to 128.
[36m(WorkerDict pid=2597122)[0m WARNING 02-03 07:59:30 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.[32m [repeated 15x across cluster][0m
[36m(vLLMHttpServer pid=2601703)[0m WARNING 02-03 07:59:46 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:46,220 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:433] => Initializing a V1 LLM engine with config: model='/mnt/share/w00914260/model/Qwen3-30B-MoE-merge', speculative_config=None, tokenizer='/mnt/share/w00914260/model/Qwen3-30B-MoE-merge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=16, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/share/w00914260/model/Qwen3-30B-MoE-merge, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer","vllm.unified_ascend_attention_with_output","vllm.mla_forward"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[256,224,200,176,144,112,88,64,32,10,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":256,"local_cache_dir":null}[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
[36m(vLLMHttpServer pid=2601703)[0m   warnings.warn(  # deprecated in 14.0 - 2024-11-09
[36m(vLLMHttpServer pid=2601703)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
[36m(vLLMHttpServer pid=2601703)[0m   from websockets.server import WebSocketServerProtocol
[36m(vLLMHttpServer pid=2601703)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py:436: ResourceWarning: unclosed <socket.socket fd=158, family=2, type=1, proto=0, laddr=('141.61.29.117', 39143)>
[36m(vLLMHttpServer pid=2601703)[0m   self._server_port, self._server_task = await run_unvicorn(app, args, self._server_address)
[36m(pid=2606084)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(pid=2606084)[0m     *************************************************************************************************************
[36m(pid=2606084)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(pid=2606084)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(pid=2606084)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(pid=2606084)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(pid=2606084)[0m     The device parameters have been replaced with npu in the function below:
[36m(pid=2606084)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(pid=2606084)[0m     *************************************************************************************************************
[36m(pid=2606084)[0m     
[36m(pid=2606084)[0m   warnings.warn(msg, ImportWarning)
[36m(pid=2606084)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(pid=2606084)[0m   warnings.warn(msg, RuntimeWarning)
[36m(pid=2606079)[0m     
[36m(pid=2606080)[0m     
[36m(pid=2606084)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(pid=2606084)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(pid=2606084)[0m   import pkg_resources
[36m(pid=2606085)[0m     
[36m(pid=2606082)[0m     
[36m(pid=2606081)[0m     
[36m(pid=2606078)[0m     
[36m(pid=2606083)[0m     
[36m(AgentLoopWorker pid=2606079)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
[36m(AgentLoopWorker pid=2606079)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
[36m(AgentLoopWorker pid=2606079)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[36m(AgentLoopWorker pid=2606079)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(AgentLoopWorker pid=2606079)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)
[36m(pid=2606083)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: [32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     *************************************************************************************************************[32m [repeated 14x across cluster][0m
[36m(pid=2606083)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     The backend in torch.distributed.init_process_group set to hccl now..[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     The device parameters have been replaced with npu in the function below:[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m   warnings.warn(msg, ImportWarning)[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m   warnings.warn(msg, RuntimeWarning)[32m [repeated 7x across cluster][0m
[36m(pid=2606083)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...[32m [repeated 7x across cluster][0m
[36m(pid=2606078)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.[32m [repeated 7x across cluster][0m
[36m(pid=2606078)[0m   import pkg_resources[32m [repeated 7x across cluster][0m
[36m(pid=2607592)[0m     
[36m(AgentLoopWorker pid=2606078)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606078)[0m <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.[32m [repeated 15x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m /mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 7x across cluster][0m
[36m(pid=2607592)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(pid=2607592)[0m     *************************************************************************************************************[32m [repeated 2x across cluster][0m
[36m(pid=2607592)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(pid=2607592)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(pid=2607592)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(pid=2607592)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(pid=2607592)[0m     The device parameters have been replaced with npu in the function below:
[36m(pid=2607592)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(pid=2607592)[0m   warnings.warn(msg, ImportWarning)
[36m(pid=2607592)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(pid=2607592)[0m   warnings.warn(msg, RuntimeWarning)
[36m(pid=2607574)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
[36m(pid=2607574)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
[36m(pid=2607574)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
[36m(pid=2607574)[0m     The backend in torch.distributed.init_process_group set to hccl now..
[36m(pid=2607574)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
[36m(pid=2607574)[0m     The device parameters have been replaced with npu in the function below:
[36m(pid=2607574)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
[36m(pid=2607574)[0m     
[36m(pid=2607574)[0m   warnings.warn(msg, ImportWarning)
[36m(pid=2607574)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
[36m(pid=2607574)[0m   warnings.warn(msg, RuntimeWarning)
[36m(pid=2607738)[0m     
[36m(pid=2607574)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...
[36m(pid=2607574)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
[36m(pid=2607574)[0m   import pkg_resources
[36m(pid=2607650)[0m     
[36m(pid=2607768)[0m     
[36m(pid=2607837)[0m     
[36m(pid=2607659)[0m     
[36m(pid=2608127)[0m     
[36m(AgentLoopWorker pid=2606078)[0m You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[36m(RewardLoopWorker pid=2608127)[0m The tokenizer you are loading from '/mnt/share/w00914260/model/Qwen3-30B-MoE-merge' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.[32m [repeated 8x across cluster][0m
[36m(pid=2608127)[0m     *************************************************************************************************************[32m [repeated 14x across cluster][0m
[36m(pid=2608127)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: [32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     The backend in torch.distributed.init_process_group set to hccl now..[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     The device parameters have been replaced with npu in the function below:[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m   warnings.warn(msg, ImportWarning)[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m   warnings.warn(msg, RuntimeWarning)[32m [repeated 6x across cluster][0m
[36m(pid=2608127)[0m Using /root/.cache/torch_extensions/py311_cpu as PyTorch extensions root...[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m /root/miniconda3/envs/verl_pt27_25rc2_0822daily/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m   import pkg_resources[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606080)[0m You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.[32m [repeated 5x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.[32m [repeated 2x across cluster][0m
[36m(TaskRunner pid=2594856)[0m Training Progress:   0%|          | 0/7005 [00:00<?, ?it/s]
[36m(TaskRunner pid=2594856)[0m Training Progress:   0%|          | 1/7005 [04:38<541:31:23, 278.34s/it]

[36m(TaskRunner pid=2594856)[0m AgentLoopManager: ['141.61.29.117:39143']
[36m(vLLMHttpServer pid=2601703)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:46,523 INFO [/mnt/share/w00914260/veomni-verl-main/huazhong/verl/verl/workers/rollout/utils.py:79] => HTTP server started on port 39143
[36m(pid=2606084)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 07:59:55 >> âœ… VeOmni ops patch applied.
[36m(pid=2606084)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 07:59:55 >> âŒ veomni_patch is not available
[36m(pid=2606084)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:34] 02/03/2026 07:59:55 >> 
[36m(pid=2606084)[0m ========== Environment Variables ==========
[36m(pid=2606084)[0m MODELING_BACKEND=veomni (source=default)
[36m(pid=2606084)[0m USE_GROUP_GEMM=1 (source=default)
[36m(pid=2606084)[0m USE_LIGER_KERNEL=1 (source=default)
[36m(pid=2606084)[0m ===========================================
[36m(pid=2606084)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 07:59:55 >> 
[36m(pid=2606084)[0m =========== OPS ============
[36m(pid=2606084)[0m _fused_moe_forward = npu_fused_moe_forward
[36m(pid=2606084)[0m _flash_attention_forward = transformers_flash_attention_forward
[36m(pid=2606084)[0m _cross_entropy = eager_cross_entropy
[36m(pid=2606084)[0m ==============================
[36m(TaskRunner pid=2594856)[0m Checkpoint tracker file does not exist: /mnt/share/w00914260/veomni-verl-main/huazhong/verl/checkpoints/verl_grpo_example_geo3k/qwen2_5_vl_3b_function_rm/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=2594856)[0m Training from scratch
[36m(TaskRunner pid=2594856)[0m test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
[36m(pid=2606084)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:55,737 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(AgentLoopWorker pid=2606080)[0m WARNING 02-03 07:59:58 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(AgentLoopWorker pid=2606080)[0m WARNING 02-03 07:59:59 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(AgentLoopWorker pid=2606080)[0m Using dataset class: RLHFDataset
[36m(pid=2607592)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 08:00:08 >> âœ… VeOmni ops patch applied.[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 08:00:08 >> âŒ veomni_patch is not available[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 08:00:08 >> [32m [repeated 16x across cluster][0m
[36m(pid=2607592)[0m ========== Environment Variables ==========[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m MODELING_BACKEND=veomni (source=default)[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m USE_LIGER_KERNEL=1 (source=default)[32m [repeated 16x across cluster][0m
[36m(pid=2607592)[0m ===========================================[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m =========== OPS ============[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m _fused_moe_forward = npu_fused_moe_forward[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m _flash_attention_forward = transformers_flash_attention_forward[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m _cross_entropy = eager_cross_entropy[32m [repeated 8x across cluster][0m
[36m(pid=2607592)[0m ==============================[32m [repeated 8x across cluster][0m
[36m(pid=2606083)[0m [Rank 0 | Local Rank 0] 2026-02-03 07:59:56,347 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606078)[0m WARNING 02-03 07:59:59 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m WARNING 02-03 07:59:59 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development![32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=2606081)[0m Using dataset class: RLHFDataset[32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=2594856)[0m validation generation end
[36m(pid=2608127)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/ops/__init__.py:50] 02/03/2026 08:00:09 >> âœ… VeOmni ops patch applied.[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:32] 02/03/2026 08:00:09 >> âŒ veomni_patch is not available[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m [INFO|/mnt/share/w00914260/veomni-verl-main/VeOmni/veomni/__init__.py:35] 02/03/2026 08:00:09 >> [32m [repeated 14x across cluster][0m
[36m(pid=2608127)[0m ========== Environment Variables ==========[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m MODELING_BACKEND=veomni (source=default)[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m USE_LIGER_KERNEL=1 (source=default)[32m [repeated 14x across cluster][0m
[36m(pid=2608127)[0m ===========================================[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m =========== OPS ============[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m _fused_moe_forward = npu_fused_moe_forward[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m _flash_attention_forward = transformers_flash_attention_forward[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m _cross_entropy = eager_cross_entropy[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m ==============================[32m [repeated 7x across cluster][0m
[36m(pid=2608127)[0m [Rank 0 | Local Rank 0] 2026-02-03 08:00:10,432 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=2594856)[0m ("Initial validation metrics: {'val-aux/openai/gsm8k/reward/mean@1': "
[36m(TaskRunner pid=2594856)[0m  "0.5269143290371494, 'val-core/openai/gsm8k/acc/mean@1': 0.5269143290371494, "
[36m(TaskRunner pid=2594856)[0m  "'val-aux/num_turns/min': 2, 'val-aux/num_turns/max': 2, "
[36m(TaskRunner pid=2594856)[0m  "'val-aux/num_turns/mean': 2.0}")
[36m(TaskRunner pid=2594856)[0m step:0 - val-aux/openai/gsm8k/reward/mean@1:0.5269143290371494 - val-core/openai/gsm8k/acc/mean@1:0.5269143290371494 - val-aux/num_turns/min:2 - val-aux/num_turns/max:2 - val-aux/num_turns/mean:2.0
[36m(TaskRunner pid=2594856)[0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[36m(WorkerDict pid=2597107)[0m ('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[36m(TaskRunner pid=2594856)[0m step:1 - global_seqlen/min:3732 - global_seqlen/max:19810 - global_seqlen/minmax_diff:16078 - global_seqlen/balanced_min:10040 - global_seqlen/balanced_max:10108 - global_seqlen/mean:10075.1875 - actor/entropy:0.14825890958309174 - perf/mfu/actor_infer:0.027699410979727595 - actor/pg_clipfrac:0.005135772495123092 - actor/ppo_kl:-3.193081445473922e-05 - actor/pg_clipfrac_lower:7.1674312493996695e-06 - actor/pg_loss:0.06835957616567612 - actor/kl_loss:0.1960113029635977 - actor/kl_coef:0.01 - actor/loss:0.0010987408459186554 - actor/grad_norm:0.6948743462562561 - actor/lr:9.999999497167352e-07 - perf/mfu/actor:0.044080369162827926 - training/global_step:1 - training/epoch:0 - critic/score/mean:0.46875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.46875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.04255307838320732 - critic/advantages/max:1.20761239528656 - critic/advantages/min:-2.4748666286468506 - critic/returns/mean:-0.04255307838320732 - critic/returns/max:1.20761239528656 - critic/returns/min:-2.4748666286468506 - response_length/mean:1167.8359375 - response_length/max:2913.0 - response_length/min:372.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:1167.8359375 - response_length_non_aborted/max:2913.0 - response_length_non_aborted/min:372.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:91.5625 - prompt_length/max:139.0 - prompt_length/min:65.0 - prompt_length/clip_ratio:0.0 - num_turns/min:2 - num_turns/max:2 - num_turns/mean:2.0 - timing_s/start_profile:0.00031511997804045677 - timing_s/agent_loop/generate_sequences/min:28.5436696500401 - timing_s/agent_loop/generate_sequences/max:215.83841579 - timing_s/agent_loop/generate_sequences/mean:87.79027782914363 - timing_s/agent_loop/tool_calls/min:0.0 - timing_s/agent_loop/tool_calls/max:0.0 - timing_s/agent_loop/tool_calls/mean:0.0 - timing_s/agent_loop/slowest/generate_sequences:215.83841579 - timing_s/agent_loop/slowest/tool_calls:0.0 - timing_s/agent_loop/slowest/prompt_length:98 - timing_s/agent_loop/slowest/response_length:2913 - timing_s/gen:236.74743231997127 - timing_s/reward:0.0001933000166900456 - timing_s/old_log_prob:11.162991200049873 - timing_s/ref:7.582088619994465 - timing_s/adv:0.005674400017596781 - timing_s/update_actor:22.529607030039188 - timing_s/step:278.0787103099865 - timing_s/stop_profile:7.475004531443119e-05 - timing_per_token_ms/adv:3.520033757186145e-05 - timing_per_token_ms/gen:1.5837749598280157 - timing_per_token_ms/update_actor:0.13975922923294967 - timing_per_token_ms/ref:0.047034413875637955 - perf/total_num_tokens:161203 - perf/time_per_step:278.0787103099865 - perf/throughput:36.23142342960649